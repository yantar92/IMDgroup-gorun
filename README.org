# -*- after-save-hook: (org-md-export-to-markdown); -*-
#+options: toc:nil
* IMDgroup-gorun

This package provides a set of scripts for supercomputer job
submission for VASP and ATAT via [[https://slurm.schedmd.com/][Slurm]]. It is tailored to research
performed in the [[https://www.oimalyi.org/][Inverse Materials Design group]], creating a unified
interface for running calculations across different high-performance
computing clusters (e.g., Athena, Ares, Helios, LUMI).

* Installation

#+begin_src bash
  git clone https://git.sr.ht/~yantar92/IMDgroup-gorun
  cd IMDgroup-gorun
  pip install .
#+end_src

* Configuration

The tool relies on specific environment variables and a TOML
configuration file to adapt to different cluster environments.

** Environment Variables

Add the following to your ~.bashrc~ or submission environment:

- ~IMDGroup~ :: Path to the directory containing configuration files
  (specifically ~$IMDGroup/dist/etc/gorun.toml~).
- ~VASP_PATH~ :: Root directory of the VASP installation. The scripts
  expect binaries at ~$VASP_PATH/bin/vasp_std~, etc.
- ~VASP_PP_PATH~ :: Path to VASP pseudopotentials (required for
  automatic POTCAR generation).
- ~CLUSTER_NAME~ :: (Optional) Manually override the cluster name
  detection (usually automatic via ~uname -n~).

** Configuration File (~gorun.toml~)

The behavior of ~gorun~ is defined in a TOML file. This file maps
hostnames to cluster definitions and specifies available queues,
modules to load, and default resource limits.

#+begin_src toml
  [cluster.names]
  lumi = ['uan01', 'uan02']

  [lumi]
  queues = ['standard', 'small']
  VASP-setup = "module load LUMI/24.03 ..."
  mpiexec = "srun"

  [lumi.standard]
  type = 'CPU'
  partition = 'standard'
  max-nodes = 512
  max-time = '48:00:00'
#+end_src

* Command Line Interface

** ~gorun~

The primary command to submit a VASP job from the current directory.

#+begin_src bash
  # Submit job requesting 2 nodes for 24 hours
  gorun 2 24:00:00

  # Submit to a specific queue
  gorun --queue plgrid-gpu-a100

  # Run locally (no sbatch) for testing
  gorun --local
#+end_src

*** Automatic Job Preparation
Before submission, ~gorun~ performs several sanity checks and
preparation steps:

- Safety Checks :: Aborts if ~INCAR~ is missing, or if a job is
  already ~RUNNING~ or queued.
- Convergence Check :: If the directory already contains a converged
  calculation, it exits to prevent wasting resources (unless
  ~--force~ is used).
- Backup :: If output exists, it backs up the current directory to
  ~gorun_N~ (incrementing N) before starting fresh.
- Input Sanitization :: Cleans ~INCAR~, ~POSCAR~, ~KPOINTS~ (fixing
  newlines, removing BOMs).
- Auto-POTCAR :: Generates ~POTCAR~ using ASE if ~POSCAR~ exists and
  ~POTCAR~ is missing.
- vdW Kernel :: Automatically copies ~vdw_kernel.bindat~ from the
  VASP source directory if required.

*** Smart Queue Selection

If no queue is specified, ~gorun~ queries the scheduler
(~sbatch --test-only~) to estimate start times for all available
partitions defined in the config. It selects the queue that offers the
*earliest finish time*.

*** Python Scripting via ~INCAR.py~

If a file named ~INCAR.py~ is present, ~gorun~ wraps the VASP
execution in a Python script using ASE. This allows dynamic
manipulation of the calculator before or after the run (e.g., for
sophisticated relaxation protocols).

** ~gorun-maps~

A wrapper to submit ATAT's ~maps~ (Cluster Expansion) code to Slurm.

#+begin_src bash
  # Run maps in current directory with specific VASP settings for the sub-jobs
  gorun-maps --kpoints=3000 --max_strain=0.1
#+end_src

This command launches ~maps~ on the compute node. It configures ~maps~
to use ~gorun-atat-local~ as the calculation script.

** ~gorun-atat-local~

The worker script used by ATAT to run individual structural
calculations. It integrates deeply with ~IMDgroup-pymatgen~ to ensure
robust calculations.

Features:
- *Derivation* : Creates VASP inputs from ATAT's ~str.out~ using the parent directory as a template (inheriting INCAR, etc.).
- *Validation*
  - Checks if atoms are too close before running.
  - Verifies K-point density (rejects grids with too few points along one axis).
  - After relaxation, checks for excessive *volume distortion* or
    *sublattice flipping*. If the structure deviates too much from the
    lattice hypothesis, it marks the run as an error to avoid
    contaminating the cluster expansion.
- *SCF*: Can optionally run a static SCF calculation after relaxation.

* Batch Submission

The package includes a companion Bash script =gorun-all-ready.sh= for submitting multiple jobs in batch. This is useful when you have many directories prepared with =gorun --mark= and want to submit them while respecting a limit on concurrent Slurm jobs.

** Usage

#+begin_src bash
  # Submit all ready directories, limiting to 4 concurrent jobs
  gorun-all-ready.sh -n 4

  # Specify a different root directory and increase verbosity
  gorun-all-ready.sh -d /path/to/project -n 8 -v
#+end_src

** How it works

1. The script recursively searches for =gorun_ready= marker files.
2. For each directory containing a marker, it checks the current number of jobs you have in the queue (via =squeue=).
3. If the count is below the limit, it runs =sbatch sub= in that directory and removes the marker.
4. If the limit is reached, it waits a configurable interval before retrying.

This ensures you never exceed your allocated job limit while automatically processing all ready calculations.

* Acknowledgements

We acknowledge financial support from the National Centre for Research
and Development (NCBR) under project
WPC3/2022/50/KEYTECH/2024. Computational resources were provided by
the Polish high-performance computing infrastructure PLGrid, including
access to the LUMI supercomputer—owned by the EuroHPC Joint
Undertaking and hosted by CSC in Finland together with the LUMI
Consortium—through allocation PLL/2024/07/017633, as well as
additional resources at the PLGrid HPC centres ACK Cyfronet AGH and
WCSS under allocation PLG/2024/017498.
